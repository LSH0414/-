{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "view_log = pd.read_csv('data/view_log.csv')\n",
    "view_log_train = view_log.copy()\n",
    "article_info = pd.read_csv('data/article_info.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# 사용자별 유니크 국가 및 지역 목록 생성\n",
    "user_countries = view_log.groupby('userID')['userCountry'].unique().apply(set)\n",
    "\n",
    "# 트레이닝 데이터에 언어 정보 병합\n",
    "view_log_with_lang = pd.merge(view_log, article_info[['articleID', 'Language']], on='articleID')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = []\n",
    "\n",
    "for user_id in view_log_with_lang['userID'].unique():\n",
    "    user_writed_articles = article_info[article_info['userID'] == user_id]['articleID'].values\n",
    "    user_view_log = view_log_with_lang[view_log_with_lang['userID'] == user_id]\n",
    "    self_view_df = user_view_log[user_view_log['articleID'].isin(user_writed_articles)]\n",
    "    self_ivew_list = self_view_df.index.to_list()\n",
    "    remain_idx = self_view_df.groupby(['userID','articleID']).head(999).index.to_list()\n",
    "    drop_idx += [idx for idx in self_ivew_list if idx not in remain_idx]\n",
    "view_log_with_lang = view_log_with_lang.drop(index=drop_idx)\n",
    "\n",
    "user_article_matrix = view_log_with_lang.groupby(['userID', 'articleID']).size().reset_index(name='visit_count')\n",
    "user_article_matrix['visit_count'] = user_article_matrix['visit_count'].apply(lambda x: min(x, 12))\n",
    "user_article_matrix['log_weight'] = user_article_matrix['visit_count'].apply(lambda x: np.log1p(x))\n",
    "\n",
    "user_article_matrix = user_article_matrix.pivot(index='userID', columns='articleID', values='log_weight').fillna(0)\n",
    "user_similarity = cosine_similarity(user_article_matrix)\n",
    "\n",
    "mean_similarity = np.mean(user_similarity[user_similarity > 0])\n",
    "additional_weight = mean_similarity * 0.79\n",
    "user_index = user_article_matrix.index\n",
    "for i, user_i in enumerate(user_index):\n",
    "    countries_i = user_countries[user_i]\n",
    "    for j, user_j in enumerate(user_index[i + 1:], start=i + 1):\n",
    "        countries_j = user_countries[user_j]\n",
    "        if countries_i & countries_j:\n",
    "            user_similarity[i, j] -= additional_weight\n",
    "            user_similarity[j, i] -= additional_weight                    \n",
    "\n",
    "person_reco_dics = {id: [] for id in view_log['userID'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_recoomand(user_article_matrix_train_random, user_similarity_train_random, person_reco_dics, df_return = False):\n",
    "    def get_language_filtered_recommendations(user_article_matrix, user_similarity):\n",
    "        user_predicted_scores = user_similarity.dot(user_article_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "        recommendations = []\n",
    "        for idx, user in enumerate(user_article_matrix.index):\n",
    "            \n",
    "            sorted_indices = user_predicted_scores[idx].argsort()[::-1]\n",
    "            \n",
    "            #---------------------------------\n",
    "            # 유사점 점수로 추천 리스트 만들기\n",
    "            top5recommend = user_article_matrix.columns[sorted_indices][:5].tolist()\n",
    "            for article in top5recommend[:5]:\n",
    "                recommendations.append([user, article])\n",
    "                \n",
    "            # if len(person_reco_dics[user]):\n",
    "            #     for embedding_reco in person_reco_dics[user]:\n",
    "            #         if embedding_reco not in top5recommend[:4]:\n",
    "            #             recommendations.append([user, embedding_reco])\n",
    "            #             break\n",
    "                \n",
    "            # if len(recommendations) % 5 != 0:\n",
    "            #     recommendations.append([user, top5recommend[-1]])\n",
    "                \n",
    "            \n",
    "        return recommendations\n",
    "\n",
    "    language_filtered_recommendations_original = get_language_filtered_recommendations(user_article_matrix_train_random, user_similarity_train_random)\n",
    "\n",
    "    # Convert recommendations to DataFrame\n",
    "    language_filtered_recommendations_original_df = pd.DataFrame(language_filtered_recommendations_original, columns=['userID', 'articleID'])\n",
    "\n",
    "    if df_return:\n",
    "        return language_filtered_recommendations_original_df\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = base_recoomand(user_article_matrix, user_similarity, person_reco_dics, df_return=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submission_14.csv')\n",
    "submission2 = pd.read_csv('submission_18.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(439, 3187, 3092)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(submission['articleID'] != df['articleID']).sum(), (submission2['articleID'] != df['articleID']).sum(), (submission['articleID'] != submission2['articleID']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['articleID'] = df['articleID']\n",
    "\n",
    "submission.to_csv('submission_24.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional_CONTRY_WEIGHT :  0.06945662379096092\n"
     ]
    }
   ],
   "source": [
    "## test\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "view_log_train = pd.read_csv('data/view_log.csv')\n",
    "article_info = pd.read_csv('data/article_info.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# 유저별로 방문 기록의 최대 20%를 넘지 않는 범위 내에서 1~10개의 기사를 랜덤하게 테스트 데이터로 선택\n",
    "def select_random_articles(group):\n",
    "    max_articles = max(1, min(5, int(len(group) * 0.2)))  # 최소 1개, 최대 10개 또는 전체의 20%\n",
    "    num_articles = np.random.randint(1, max_articles + 1)  # 랜덤 선택 개수 결정\n",
    "    return np.random.choice(group.index, num_articles, replace=False)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# np.random.seed(1111)\n",
    "test_indices = view_log_train.drop_duplicates(subset=['userID',\t'articleID', 'userRegion',\t'userCountry']).groupby('userID').apply(select_random_articles).explode().astype(int)\n",
    "test_data_random = view_log_train.loc[test_indices]\n",
    "train_data_random = view_log_train.drop(test_indices)\n",
    "\n",
    "drop_idx = []\n",
    "for user_id in train_data_random['userID'].unique():\n",
    "    user_writed_articles = article_info[article_info['userID'] == user_id]['articleID'].values\n",
    "    user_view_log = train_data_random[train_data_random['userID'] == user_id]\n",
    "    self_view_df = user_view_log[user_view_log['articleID'].isin(user_writed_articles)]\n",
    "    self_ivew_list = self_view_df.index.to_list()\n",
    "    remain_idx = self_view_df.groupby(['userID','articleID']).head(20).index.to_list()\n",
    "    drop_idx += [idx for idx in self_ivew_list if idx not in remain_idx]\n",
    "train_data_random = train_data_random.drop(index=drop_idx)\n",
    "\n",
    "\n",
    "# 사용자별 유니크 국가 및 지역 목록 생성\n",
    "user_countries = view_log_train.groupby('userID')['userCountry'].unique().apply(set)\n",
    "\n",
    "# 테스트 데이터 준비\n",
    "test_data_for_comparison = test_data_random[['userID', 'articleID']].drop_duplicates()\n",
    "\n",
    "\n",
    "user_article_matrix_train_random = train_data_random.groupby(['userID', 'articleID']).size().reset_index(name='visit_count')\n",
    "user_article_matrix_train_random['visit_count'] = user_article_matrix_train_random['visit_count'].apply(lambda x: min(x, 10))\n",
    "user_article_matrix_train_random['log_weight'] = user_article_matrix_train_random['visit_count'].apply(lambda x: np.log1p(x))\n",
    "user_article_matrix_train_random = user_article_matrix_train_random.pivot(index='userID', columns='articleID', values='log_weight').fillna(0)\n",
    "\n",
    "# 사용자 간의 유사성 계산\n",
    "user_similarity_train_random = cosine_similarity(user_article_matrix_train_random)\n",
    "\n",
    "\n",
    "mean_similarity = np.mean(user_similarity_train_random[user_similarity_train_random > 0])\n",
    "\n",
    "# 평균의 10%를 추가 가중치로 설정\n",
    "additional_weight = mean_similarity * 0.8\n",
    "print('Additional_CONTRY_WEIGHT : ', additional_weight)\n",
    "user_index = user_article_matrix_train_random.index\n",
    "for i, user_i in enumerate(user_index):\n",
    "    countries_i = user_countries[user_i]\n",
    "    for j, user_j in enumerate(user_index[i+1:], start=i+1):  # 대칭성을 고려하여 j를 i+1부터 시작\n",
    "        countries_j = user_countries[user_j]\n",
    "        if countries_i & countries_j:  # 두 사용자가 하나 이상의 공통 국가를 가지고 있는 경우\n",
    "            user_similarity_train_random[i, j] -= (additional_weight)\n",
    "            user_similarity_train_random[j, i] -= (additional_weight)  # 유사성 행렬은 대칭이므로\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10837755875663381"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_recoomand(user_article_matrix_train_random, user_similarity_train_random, df_return = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|no|score|score2|score3|socre4|\n",
    "|---|------|------|-----|----|\n",
    "|1|0.10842810209754863 | 0.11132170836492293|0.10674753601213041|0.10607783674500884|\n",
    "|2|0.1083522870861764 | 0.11237048268890576 |0.11148597422289615|0.10985595147839272\n",
    "|3|0.10803639120545869 | 0.11511245893353549|0.10816274955774578|0.10881981298963861\n",
    "|4|0.1090725296942128 | 0.11536517563810969|0.10839019459186253|0.10812484205205965\n",
    "|5|0.10903462218852666 | 0.11328026282537278|0.10820065706343189|0.10812484205205965\n",
    "|6|0.10903462218852666 | 0.11328026282537278|0.10839019459186253|0.10812484205205965\n",
    "|7|0.10903462218852666 | 0.11328026282537278|0.10839019459186253|0.10837755875663381\n",
    "|8|0.10827647207480415 | 0.11328026282537278|0.10839019459186253|0.10818802122820317\n",
    "|9|0.1086555471316654 |  0.11328026282537278|0.10839019459186253|0.10818802122820317\n",
    "|10|0.10941369724538792 | 0.1140384129390953|0.10839019459186253|0.10837755875663381\n",
    "|11|0.10941369724538792 |\n",
    "|12|0.10941369724538792 |\n",
    "|13|0.10941369724538792 |\n",
    "\n",
    "---\n",
    "0.10941369724538792"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall@5 계산 함수\n",
    "def recall5(answer_df, submission_df):\n",
    "    primary_col = answer_df.columns[0]\n",
    "    secondary_col = answer_df.columns[1]\n",
    "    prediction_counts = submission_df.groupby(primary_col).size()\n",
    "    if not all(prediction_counts == 5):\n",
    "        raise ValueError(f\"Each {primary_col} should have exactly 5 {secondary_col} predictions.\")\n",
    "    if submission_df[secondary_col].isnull().any():\n",
    "        raise ValueError(f\"Predicted {secondary_col} contains NULL values.\")\n",
    "    duplicated_preds = submission_df.groupby(primary_col).apply(lambda x: x[secondary_col].duplicated().any())\n",
    "    if duplicated_preds.any():\n",
    "        raise ValueError(f\"Predicted {secondary_col} contains duplicates for some {primary_col}.\")\n",
    "    submission_df = submission_df[submission_df[primary_col].isin(answer_df[primary_col])]\n",
    "    top_5_preds = submission_df.groupby(primary_col).apply(lambda x: x[secondary_col].head(5).tolist()).to_dict()\n",
    "    true_dict = answer_df.groupby(primary_col).apply(lambda x: x[secondary_col].tolist()).to_dict()\n",
    "    individual_recalls = [len(set(true_dict[key]) & set(top_5_preds[key])) / min(len(val), 5) for key, val in true_dict.items() if key in top_5_preds]\n",
    "    recall = np.mean(individual_recalls) if individual_recalls else 0\n",
    "    return recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional_CONTRY_WEIGHT :  0.06977574562848603\n"
     ]
    }
   ],
   "source": [
    "# 사용자 간의 유사성 계산\n",
    "user_similarity_train_random = cosine_similarity(user_article_matrix_train_random)\n",
    "\n",
    "\n",
    "mean_similarity = np.mean(user_similarity_train_random[user_similarity_train_random > 0])\n",
    "\n",
    "# 평균의 10%를 추가 가중치로 설정\n",
    "additional_weight = mean_similarity * 1\n",
    "print('Additional_CONTRY_WEIGHT : ', additional_weight)\n",
    "user_index = user_article_matrix_train_random.index\n",
    "for i, user_i in enumerate(user_index):\n",
    "    countries_i = user_countries[user_i]\n",
    "    for j, user_j in enumerate(user_index[i+1:], start=i+1):  # 대칭성을 고려하여 j를 i+1부터 시작\n",
    "        countries_j = user_countries[user_j]\n",
    "        if countries_i & countries_j:  # 두 사용자가 하나 이상의 공통 국가를 가지고 있는 경우\n",
    "            user_similarity_train_random[i, j] -= (additional_weight)\n",
    "            user_similarity_train_random[j, i] -= (additional_weight)  # 유사성 행렬은 대칭이므로\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_recoomand(user_article_matrix_train_random, user_similarity_train_random, df_return = False):\n",
    "    def get_language_filtered_recommendations(user_article_matrix, user_similarity):\n",
    "        user_predicted_scores = user_similarity.dot(user_article_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "        recommendations = []\n",
    "        for idx, user in enumerate(user_article_matrix.index):\n",
    "            \n",
    "            sorted_indices = user_predicted_scores[idx].argsort()[::-1]\n",
    "            top5recommend = user_article_matrix.columns[sorted_indices][:5].tolist()\n",
    "            \n",
    "            #---------------------------------\n",
    "            # 유사점 점수를 먼저 추가하는 코드\n",
    "            for article in top5recommend[:5]:\n",
    "                recommendations.append([user, article])\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    language_filtered_recommendations_original = get_language_filtered_recommendations(user_article_matrix_train_random, user_similarity_train_random)\n",
    "\n",
    "    # Convert recommendations to DataFrame\n",
    "    language_filtered_recommendations_original_df = pd.DataFrame(language_filtered_recommendations_original, columns=['userID', 'articleID'])\n",
    "\n",
    "    if df_return:\n",
    "        print(recall5(test_data_for_comparison, language_filtered_recommendations_original_df))\n",
    "        return language_filtered_recommendations_original_df\n",
    "        \n",
    "    # Calculate Recall@5 for the original algorithm's recommendations\n",
    "    recall_at_5_original = recall5(test_data_for_comparison, language_filtered_recommendations_original_df)\n",
    "    return recall_at_5_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1889941875157948"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_recoomand(user_article_matrix_train_random, user_similarity_train_random, df_return = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2002906242102603\n",
    "0.20434672731867573\n",
    "0.20648218347232755\n",
    "0.20703816022239072\n",
    "0.20597675006317917\n",
    "0.2058503917108921\n",
    "0.2058503917108921\n",
    "0.20827647207480413\n",
    "0.20903462218852664\n",
    "0.20903462218852664\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twigfarm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
