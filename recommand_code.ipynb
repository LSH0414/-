{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "view_log_train = pd.read_csv('data/view_log.csv')\n",
    "article_info = pd.read_csv('data/article_info.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# 유저별로 방문 기록의 최대 20%를 넘지 않는 범위 내에서 1~10개의 기사를 랜덤하게 테스트 데이터로 선택\n",
    "def select_random_articles(group):\n",
    "    max_articles = max(1, min(10, int(len(group) * 0.2)))  # 최소 1개, 최대 10개 또는 전체의 20%\n",
    "    num_articles = np.random.randint(1, max_articles + 1)  # 랜덤 선택 개수 결정\n",
    "    return np.random.choice(group.index, num_articles, replace=False)\n",
    "\n",
    "# np.random.seed(42)\n",
    "\n",
    "np.random.seed(42)\n",
    "test_indices = view_log_train.groupby('userID').apply(select_random_articles).explode().astype(int)\n",
    "test_data_random = view_log_train.loc[test_indices]\n",
    "train_data_random = view_log_train.drop(test_indices)\n",
    "\n",
    "# 사용자별 유니크 국가 및 지역 목록 생성\n",
    "user_countries = view_log_train.groupby('userID')['userCountry'].unique().apply(set)\n",
    "\n",
    "# 테스트 데이터 준비\n",
    "test_data_for_comparison = test_data_random[['userID', 'articleID']].drop_duplicates()\n",
    "\n",
    "# 트레이닝 데이터에 언어 정보 병합\n",
    "train_data_with_lang_random = pd.merge(train_data_random, article_info[['articleID', 'Language']], on='articleID')\n",
    "\n",
    "# 사용자-기사 행렬 생성\n",
    "user_article_matrix_train_random = train_data_random.groupby(['userID', 'articleID']).size().unstack(fill_value=0)\n",
    "\n",
    "# user_article_matrix_train_random = train_data_random.groupby(['userID', 'articleID']).size().reset_index(name='visit_count')\n",
    "# user_article_matrix_train_random['log_weight'] = user_article_matrix_train_random['visit_count'].apply(lambda x: np.log1p(x))\n",
    "# user_article_matrix_train_random = user_article_matrix_train_random.pivot(index='userID', columns='articleID', values='log_weight').fillna(0)\n",
    "\n",
    "# 사용자 간의 유사성 계산\n",
    "user_similarity_train_random = cosine_similarity(user_article_matrix_train_random)\n",
    "\n",
    "\n",
    "mean_similarity = np.mean(user_similarity_train_random[user_similarity_train_random > 0])\n",
    "\n",
    "# 평균의 10%를 추가 가중치로 설정\n",
    "additional_weight = mean_similarity * 0.2\n",
    "print('Additional_CONTRY_WEIGHT : ', additional_weight)\n",
    "user_index = user_article_matrix_train_random.index\n",
    "for i, user_i in enumerate(user_index):\n",
    "    countries_i = user_countries[user_i]\n",
    "    for j, user_j in enumerate(user_index[i+1:], start=i+1):  # 대칭성을 고려하여 j를 i+1부터 시작\n",
    "        countries_j = user_countries[user_j]\n",
    "        if countries_i & countries_j:  # 두 사용자가 하나 이상의 공통 국가를 가지고 있는 경우\n",
    "            user_similarity_train_random[i, j] -= (additional_weight)\n",
    "            user_similarity_train_random[j, i] -= (additional_weight)  # 유사성 행렬은 대칭이므로\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall@5 계산 함수\n",
    "def recall5(answer_df, submission_df):\n",
    "    primary_col = answer_df.columns[0]\n",
    "    secondary_col = answer_df.columns[1]\n",
    "    prediction_counts = submission_df.groupby(primary_col).size()\n",
    "    if not all(prediction_counts == 5):\n",
    "        raise ValueError(f\"Each {primary_col} should have exactly 5 {secondary_col} predictions.\")\n",
    "    if submission_df[secondary_col].isnull().any():\n",
    "        raise ValueError(f\"Predicted {secondary_col} contains NULL values.\")\n",
    "    duplicated_preds = submission_df.groupby(primary_col).apply(lambda x: x[secondary_col].duplicated().any())\n",
    "    if duplicated_preds.any():\n",
    "        raise ValueError(f\"Predicted {secondary_col} contains duplicates for some {primary_col}.\")\n",
    "    submission_df = submission_df[submission_df[primary_col].isin(answer_df[primary_col])]\n",
    "    top_5_preds = submission_df.groupby(primary_col).apply(lambda x: x[secondary_col].head(5).tolist()).to_dict()\n",
    "    true_dict = answer_df.groupby(primary_col).apply(lambda x: x[secondary_col].tolist()).to_dict()\n",
    "    individual_recalls = [len(set(true_dict[key]) & set(top_5_preds[key])) / min(len(val), 5) for key, val in true_dict.items() if key in top_5_preds]\n",
    "    recall = np.mean(individual_recalls) if individual_recalls else 0\n",
    "    return recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('embedding_recoomand.json', 'r') as f:\n",
    "    embedding_reco_top2 = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언어 분리하여 추천 목록 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다른 언어를 더 많이 봤는지 체크\n",
    "\n",
    "- 소수 언어 기사에 대해서 로그의 10% 비중을 넘는다면 해당 언어 기사는 추천해줄 만하다?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lang_recommand = {user_id : [] for user_id in submission['userID'].unique()}\n",
    "# def check_recoomand_language(df, filter_lang, lang_recommand, lang_thershold = 0.1):\n",
    "#     reco_lang_result = dict()\n",
    "#     for user_id in df['userID'].unique():\n",
    "        \n",
    "#         value_counts = view_log_concat_lang[view_log_concat_lang['userID'] == user_id]['Language'].value_counts()\n",
    "#         view_articles_cnt = value_counts.sum()\n",
    "        \n",
    "#         for idx, result in enumerate(value_counts/view_articles_cnt > lang_thershold):\n",
    "#             lang = value_counts.index[idx]\n",
    "\n",
    "#             if result:\n",
    "#                 if lang not in lang_recommand[user_id]:\n",
    "#                     lang_recommand[user_id].append(lang)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_recoomand(user_article_matrix_train_random, user_similarity_train_random, df_return = False):\n",
    "    def get_language_filtered_recommendations(user_article_matrix, user_similarity, significant_languages):\n",
    "        user_predicted_scores = user_similarity.dot(user_article_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "        recommendations = []\n",
    "        for idx, user in enumerate(user_article_matrix.index):\n",
    "            \n",
    "            sorted_indices = user_predicted_scores[idx].argsort()[::-1]\n",
    "            top5recommend = user_article_matrix.columns[sorted_indices][:5].tolist()\n",
    "            \n",
    "            #---------------------------------\n",
    "            # 유사점 점수를 먼저 추가하는 코드\n",
    "            \n",
    "            for article in top5recommend[:5]:\n",
    "                recommendations.append([user, article])\n",
    "                \n",
    "            # if len(embedding_reco_top2[user]):\n",
    "            #     for embedding_reco in embedding_reco_top2[user]:\n",
    "            #         if embedding_reco not in top5recommend[:4]:\n",
    "            #             recommendations.append([user, embedding_reco])\n",
    "            #             break\n",
    "                \n",
    "            # if len(recommendations) % 5 != 0:\n",
    "            #     recommendations.append([user, top5recommend[-1]])\n",
    "            #---------------------------------\n",
    "            \n",
    "            #---------------------------------\n",
    "            # 임베딩 점수를 먼저 추가하는 코드\n",
    "            # if len(embedding_reco_top2[user]):\n",
    "            #     for embedding_reco in embedding_reco_top2[user]:\n",
    "            #         recommendations.append([user, embedding_reco])\n",
    "            \n",
    "            # if len(recommendations) % 5 != 0 or len(embedding_reco_top2[user]) == 0:\n",
    "            #     top5recommend = user_article_matrix.columns[sorted_indices][:5].tolist()\n",
    "            #     for article in top5recommend:\n",
    "            #         if article not in embedding_reco_top2[user]:\n",
    "            #             recommendations.append([user, article])\n",
    "                        \n",
    "            #         if len(recommendations) % 5 == 0:\n",
    "            #             break\n",
    "            #---------------------------------\n",
    "            \n",
    "            \n",
    "            # recommendations.append([user, 'ARTICLE_99998'])\n",
    "        return recommendations\n",
    "\n",
    "    language_filtered_recommendations_original = get_language_filtered_recommendations(user_article_matrix_train_random, user_similarity_train_random, user_significant_languages_original)\n",
    "\n",
    "    # Convert recommendations to DataFrame\n",
    "    language_filtered_recommendations_original_df = pd.DataFrame(language_filtered_recommendations_original, columns=['userID', 'articleID'])\n",
    "\n",
    "    if df_return:\n",
    "        return language_filtered_recommendations_original_df\n",
    "        \n",
    "    # Calculate Recall@5 for the original algorithm's recommendations\n",
    "    recall_at_5_original = recall5(test_data_for_comparison, language_filtered_recommendations_original_df)\n",
    "    return recall_at_5_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_recoomand(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random)\n",
    "#0.2095653272681324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base - 0.19732120293151376\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 임베딩 임계\n",
    "- 0.5 -> 0.21183977760929995\n",
    "- 0.45 -> 0.21183977760929995\n",
    "- 0.4 -> 0.21259792772302247\n",
    "- 0.35 -> 0.21259792772302247\n",
    "- 0.3 -> 0.2123452110184483\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5번 빼기 0.18908263836239575\n",
    "# 4번 빼기 0.18726307808946172\n",
    "# 3번 빼기 0.1725676017184736\n",
    "# 2번 빼기 0.16897902451352034\n",
    "# 1번 빼끼 0.1294667677533485\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 독점 언어만으로 추천"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_top_lang_reocommand(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random):\n",
    "    def check_recoomand_language_variant(df, lang_threshold=0.3):\n",
    "        user_languages = {}\n",
    "        for user_id in df['userID'].unique():\n",
    "            user_data = df[df['userID'] == user_id]\n",
    "            value_counts = user_data['Language'].value_counts()\n",
    "            total_views = value_counts.sum()\n",
    "            language_ratio = value_counts / total_views\n",
    "            dominant_language = language_ratio[language_ratio > lang_threshold]\n",
    "            if not dominant_language.empty:\n",
    "                user_languages[user_id] = dominant_language.idxmax()  # Assign the single dominant language\n",
    "            else:\n",
    "                significant_languages = value_counts[value_counts / total_views > 0.1].index.tolist()\n",
    "                user_languages[user_id] = significant_languages\n",
    "        return user_languages\n",
    "\n",
    "    user_significant_languages_variant = check_recoomand_language_variant(train_data_with_lang_random)\n",
    "\n",
    "    def get_language_filtered_recommendations_variant(user_article_matrix, user_similarity, significant_languages):\n",
    "        user_predicted_scores = user_similarity.dot(user_article_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "        recommendations = []\n",
    "        for idx, user in enumerate(user_article_matrix.index):\n",
    "            if user in significant_languages:\n",
    "                user_langs = [significant_languages[user]] if isinstance(significant_languages[user], str) else significant_languages[user]\n",
    "                articles_filtered = user_article_matrix.columns.intersection(article_info[article_info['Language'].isin(user_langs)]['articleID'])\n",
    "                filtered_indices = user_article_matrix.columns.get_indexer(articles_filtered)\n",
    "                sorted_indices = user_predicted_scores[idx, filtered_indices].argsort()[::-1]\n",
    "                top5recommend = articles_filtered.values[sorted_indices][:5]\n",
    "                for article in top5recommend:\n",
    "                    recommendations.append([user, article])\n",
    "        return recommendations\n",
    "\n",
    "    language_filtered_recommendations_variant = get_language_filtered_recommendations_variant(user_article_matrix_train_random, user_similarity_train_random, user_significant_languages_variant)\n",
    "\n",
    "    language_filtered_recommendations_variant_df = pd.DataFrame(language_filtered_recommendations_variant, columns=['userID', 'articleID'])\n",
    "\n",
    "    recall_at_5_variant = recall5(test_data_for_comparison, language_filtered_recommendations_variant_df)\n",
    "    return recall_at_5_variant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_top_lang_reocommand(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 소수 언어 포함시 무조건 해당 언어 기사 들어가도록 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_minority_articles(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random, article_info):\n",
    "\n",
    "    language_counts = article_info['Language'].value_counts()\n",
    "    minority_filter = language_counts/language_counts.sum() < 0.1  # 하위 10% 언어를 소수 언어로 가정\n",
    "    minority_languages = language_counts[minority_filter].index\n",
    "\n",
    "    def check_recommended_language(df, lang_threshold=0.01):\n",
    "        value_counts = df['Language'].value_counts()\n",
    "        total_views = value_counts.sum()\n",
    "        minority_filter = value_counts/total_views < 0.1\n",
    "        significant_languages = value_counts[minority_filter].index.tolist()\n",
    "        \n",
    "        user_languages = {}\n",
    "        for user_id in df['userID'].unique():\n",
    "            user_data = df[df['userID'] == user_id]\n",
    "            value_counts = user_data['Language'].value_counts()\n",
    "            total_views = value_counts.sum()\n",
    "            minority_filter = value_counts/total_views < 0.1\n",
    "            user_significant_languages = [lang for lang in value_counts[minority_filter].index.tolist() if lang in significant_languages]\n",
    "            user_languages[user_id] = user_significant_languages\n",
    "        return user_languages\n",
    "\n",
    "    def get_language_filtered_recommendations_with_fallback_and_minority(user_article_matrix, user_similarity, significant_languages, user_index, article_info, minority_languages):\n",
    "        user_predicted_scores = user_similarity.dot(user_article_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "        recommendations = []\n",
    "        global_top_articles = article_info['articleID'].value_counts().head(5).index.tolist()\n",
    "\n",
    "        for user in user_index:\n",
    "            recommended_articles = []\n",
    "            user_langs = significant_languages.get(user, [])\n",
    "\n",
    "            # 사용자의 중요 언어를 기반으로 추천\n",
    "            if user_langs:\n",
    "                articles_filtered = user_article_matrix.columns.intersection(article_info[article_info['Language'].isin(user_langs)]['articleID'])\n",
    "                filtered_indices = user_article_matrix.columns.get_indexer(articles_filtered)\n",
    "                sorted_indices = user_predicted_scores[user_article_matrix.index.get_loc(user), filtered_indices].argsort()[::-1]\n",
    "                recommended_articles.extend(articles_filtered.values[sorted_indices][:5])\n",
    "\n",
    "                # 중요 언어 중 소수 언어가 있을 경우 그 언어의 기사를 포함\n",
    "                for lang in set(user_langs).intersection(minority_languages):\n",
    "                    minority_articles = article_info[article_info['Language'] == lang]['articleID'].values\n",
    "                    # 최소 한 개의 소수 언어 기사를 추가\n",
    "                    for article in minority_articles:\n",
    "                        if article not in recommended_articles:\n",
    "                            recommended_articles.insert(0, article)  # 최상위에 추가\n",
    "                            break\n",
    "\n",
    "            # 기사 수가 충분하지 않은 경우 일반적으로 인기 있는 기사 추가\n",
    "            while len(recommended_articles) < 5:\n",
    "                for article in global_top_articles:\n",
    "                    if article not in recommended_articles:\n",
    "                        recommended_articles.append(article)\n",
    "                        if len(recommended_articles) == 5:\n",
    "                            break\n",
    "\n",
    "            recommendations.extend([(user, article) for article in recommended_articles[:5]])\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    # 추천 로직 실행\n",
    "    user_significant_languages_train_random = check_recommended_language(train_data_with_lang_random)\n",
    "    test_user_index_random = test_data_random['userID'].unique()\n",
    "    language_filtered_recommendations_random = get_language_filtered_recommendations_with_fallback_and_minority(\n",
    "        user_article_matrix_train_random, user_similarity_train_random, user_significant_languages_train_random, test_user_index_random, article_info, minority_languages\n",
    "    )\n",
    "\n",
    "    # 결과 DataFrame으로 변환\n",
    "    language_filtered_recommendations_random_df = pd.DataFrame(language_filtered_recommendations_random, columns=['userID', 'articleID'])\n",
    "\n",
    "    # Calculate Recall@5 for the modified recommendations including minority language articles\n",
    "    recall_at_5_with_minority = recall5(test_data_for_comparison, language_filtered_recommendations_random_df)\n",
    "    return recall_at_5_with_minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_minority_articles(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random, article_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socres = {\n",
    "    'weight' : [],\n",
    "    'case1' : [],\n",
    "    'case2' : [],\n",
    "    'case3' : [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "for idx in tqdm(range(0, 101)):\n",
    "    # 사용자 간의 유사성 계산\n",
    "    user_similarity_train_random = cosine_similarity(user_article_matrix_train_random)\n",
    "\n",
    "\n",
    "    mean_similarity = np.mean(user_similarity_train_random[user_similarity_train_random > 0])\n",
    "\n",
    "    additional_weight = mean_similarity * (idx * 0.5)\n",
    "    print('Additional_WEIGHT : ', additional_weight)\n",
    "    user_index = user_article_matrix_train_random.index\n",
    "    for i, user_i in enumerate(user_index):\n",
    "        countries_i = user_countries[user_i]\n",
    "        for j, user_j in enumerate(user_index[i+1:], start=i+1):  # 대칭성을 고려하여 j를 i+1부터 시작\n",
    "            countries_j = user_countries[user_j]\n",
    "            if countries_i & countries_j:  # 두 사용자가 하나 이상의 공통 국가를 가지고 있는 경우\n",
    "                user_similarity_train_random[i, j] += additional_weight\n",
    "                user_similarity_train_random[j, i] += additional_weight  # 유사성 행렬은 대칭이므로\n",
    "\n",
    "\n",
    "    case1_score = base_recoomand(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random)\n",
    "    case2_score = only_top_lang_reocommand(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random)\n",
    "    case3_score = add_minority_articles(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random, article_info)\n",
    "\n",
    "    socres['weight'].append(idx)\n",
    "    socres['case1'].append(case1_score)\n",
    "    socres['case2'].append(case2_score)\n",
    "    socres['case3'].append(case3_score)\n",
    "\n",
    "    with open('scores.json', 'w') as f:\n",
    "            json.dump(socres, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socres = {\n",
    "    'weight' : [],\n",
    "    'case1' : [],\n",
    "    # 'case2' : [],\n",
    "    # 'case3' : [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "for idx in tqdm(range(0, 50)):\n",
    "    # 사용자 간의 유사성 계산\n",
    "    user_similarity_train_random = cosine_similarity(user_article_matrix_train_random)\n",
    "\n",
    "\n",
    "    mean_similarity = np.mean(user_similarity_train_random[user_similarity_train_random > 0])\n",
    "\n",
    "    # 평균의 10%를 추가 가중치로 설정\n",
    "    additional_weight = mean_similarity * 0.2\n",
    "    additional_region_weight = mean_similarity * (idx * 0.05)\n",
    "    print('Additional_CONTRY_WEIGHT : ', additional_weight, 'Additional_REGION_WEIGHT: ', additional_region_weight)\n",
    "    user_index = user_article_matrix_train_random.index\n",
    "    for i, user_i in enumerate(user_index):\n",
    "        countries_i = user_countries[user_i]\n",
    "        regions_i = user_regions[user_i]\n",
    "        for j, user_j in enumerate(user_index[i+1:], start=i+1):  # 대칭성을 고려하여 j를 i+1부터 시작\n",
    "            countries_j = user_countries[user_j]\n",
    "            regions_j = user_regions[user_j]\n",
    "\n",
    "            if countries_i & countries_j:  # 두 사용자가 하나 이상의 공통 국가를 가지고 있는 경우\n",
    "                user_similarity_train_random[i, j] -= (additional_weight)\n",
    "                user_similarity_train_random[j, i] -= (additional_weight)\n",
    "            \n",
    "            if regions_i & regions_j:\n",
    "                user_similarity_train_random[i, j] += (additional_region_weight)\n",
    "                user_similarity_train_random[j, i] += (additional_region_weight)\n",
    "                \n",
    "\n",
    "\n",
    "    case1_score = base_recoomand(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random)\n",
    "    # case2_score = only_top_lang_reocommand(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random)\n",
    "    # case3_score = add_minority_articles(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random, article_info)\n",
    "\n",
    "    socres['weight'].append(idx)\n",
    "    socres['case1'].append(case1_score)\n",
    "    # socres['case2'].append(case2_score)\n",
    "    # socres['case3'].append(case3_score)\n",
    "\n",
    "    with open('scores_add_region.json', 'w') as f:\n",
    "            json.dump(socres, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 트레이닝 데이터에 언어 정보 병합\n",
    "view_log_with_lang = pd.merge(view_log_train, article_info[['articleID', 'Language']], on='articleID')\n",
    "\n",
    "# 사용자-기사 행렬 생성\n",
    "user_article_matrix = view_log_train.groupby(['userID', 'articleID']).size().unstack(fill_value=0)\n",
    "\n",
    "# 사용자 간의 유사성 계산\n",
    "user_similarity= cosine_similarity(user_article_matrix)\n",
    "\n",
    "\n",
    "mean_similarity = np.mean(user_similarity[user_similarity > 0])\n",
    "\n",
    "# 평균의 10%를 추가 가중치로 설정\n",
    "additional_weight = mean_similarity * 0.2\n",
    "print('Additional_CONTRY_WEIGHT : ', additional_weight)\n",
    "user_index = user_article_matrix.index\n",
    "for i, user_i in enumerate(user_index):\n",
    "    countries_i = user_countries[user_i]\n",
    "    for j, user_j in enumerate(user_index[i+1:], start=i+1):  # 대칭성을 고려하여 j를 i+1부터 시작\n",
    "        countries_j = user_countries[user_j]\n",
    "        if countries_i & countries_j:  # 두 사용자가 하나 이상의 공통 국가를 가지고 있는 경우\n",
    "            user_similarity[i, j] -= (additional_weight)\n",
    "            user_similarity[j, i] -= (additional_weight)  # 유사성 행렬은 대칭이므로\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = base_recoomand(view_log_with_lang, user_article_matrix, user_similarity, df_return=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "view_log = pd.read_csv('data/view_log.csv')\n",
    "article_info = pd.read_csv('data/article_info.csv')\n",
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "\n",
    "# with open('title_relevance_docs_top10.json', 'r') as f:\n",
    "#     title_relvance_score_dict = json.load(f)\n",
    "\n",
    "\n",
    "# Recall@5 계산 함수\n",
    "def recall5(answer_df, submission_df):\n",
    "    primary_col = answer_df.columns[0]\n",
    "    secondary_col = answer_df.columns[1]\n",
    "    prediction_counts = submission_df.groupby(primary_col).size()\n",
    "    if not all(prediction_counts == 5):\n",
    "        raise ValueError(f\"Each {primary_col} should have exactly 5 {secondary_col} predictions.\")\n",
    "    if submission_df[secondary_col].isnull().any():\n",
    "        raise ValueError(f\"Predicted {secondary_col} contains NULL values.\")\n",
    "    duplicated_preds = submission_df.groupby(primary_col).apply(lambda x: x[secondary_col].duplicated().any())\n",
    "    if duplicated_preds.any():\n",
    "        raise ValueError(f\"Predicted {secondary_col} contains duplicates for some {primary_col}.\")\n",
    "    submission_df = submission_df[submission_df[primary_col].isin(answer_df[primary_col])]\n",
    "    top_5_preds = submission_df.groupby(primary_col).apply(lambda x: x[secondary_col].head(5).tolist()).to_dict()\n",
    "    true_dict = answer_df.groupby(primary_col).apply(lambda x: x[secondary_col].tolist()).to_dict()\n",
    "    individual_recalls = [len(set(true_dict[key]) & set(top_5_preds[key])) / min(len(val), 5) for key, val in true_dict.items() if key in top_5_preds]\n",
    "    recall = np.mean(individual_recalls) if individual_recalls else 0\n",
    "    return recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_recoomand(user_article_matrix_train_random, user_similarity_train_random, person_reco_dics, test_data_for_comparison, df_return = False):\n",
    "    def get_language_filtered_recommendations(user_article_matrix, user_similarity, person_reco_dics):\n",
    "        user_predicted_scores = user_similarity.dot(user_article_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "        recommendations = []\n",
    "        for idx, user in enumerate(user_article_matrix.index):\n",
    "            \n",
    "            sorted_indices = user_predicted_scores[idx].argsort()[::-1]\n",
    "            \n",
    "            \n",
    "            #---------------------------------\n",
    "            # 유사점 점수를 먼저 추가하는 코드\n",
    "            top5recommend = user_article_matrix.columns[sorted_indices][:5].tolist()\n",
    "            for article in top5recommend[:5]:\n",
    "                recommendations.append([user, article])\n",
    "                \n",
    "        return recommendations\n",
    "\n",
    "    language_filtered_recommendations_original = get_language_filtered_recommendations(user_article_matrix_train_random, user_similarity_train_random, person_reco_dics)\n",
    "\n",
    "    # Convert recommendations to DataFrame\n",
    "    language_filtered_recommendations_original_df = pd.DataFrame(language_filtered_recommendations_original, columns=['userID', 'articleID'])\n",
    "\n",
    "    if df_return:\n",
    "        return language_filtered_recommendations_original_df\n",
    "        \n",
    "    # Calculate Recall@5 for the original algorithm's recommendations\n",
    "    recall_at_5_original = recall5(test_data_for_comparison, language_filtered_recommendations_original_df)\n",
    "    return recall_at_5_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_minority_articles(train_data_with_lang_random, user_article_matrix_train_random, user_similarity_train_random, article_info, person_reco_dics, test_data_for_comparison, lang_thresholds, train_data_random, df_return = False):\n",
    "\n",
    "    language_counts = article_info['Language'].value_counts()\n",
    "    minority_filter = language_counts/language_counts.sum() < 0.1  # 하위 10% 언어를 소수 언어로 가정\n",
    "    minority_languages = language_counts[minority_filter].index\n",
    "\n",
    "    def check_recommended_language(df, minority_languages, lang_threshold=0.01):\n",
    "        \n",
    "        user_languages = {}\n",
    "        for user_id in df['userID'].unique():\n",
    "            user_data = df[df['userID'] == user_id]\n",
    "            value_counts = user_data['Language'].value_counts()\n",
    "            total_views = value_counts.sum()\n",
    "            minority_filter = value_counts/total_views < lang_threshold\n",
    "            if len(value_counts[minority_filter].index.tolist()):\n",
    "                user_languages[user_id] = []\n",
    "                continue\n",
    "            user_significant_languages = [lang for lang in value_counts[minority_filter].index.tolist() if lang in minority_languages]\n",
    "            user_languages[user_id] = user_significant_languages\n",
    "        return user_languages\n",
    "\n",
    "    def get_language_filtered_recommendations_with_fallback_and_minority(user_article_matrix, user_similarity, significant_languages, article_info, person_reco_dics, train_data_random):\n",
    "        user_predicted_scores = user_similarity.dot(user_article_matrix) / np.array([np.abs(user_similarity).sum(axis=1)]).T\n",
    "        recommendations = []\n",
    "        \n",
    "\n",
    "        for idx, user in enumerate(user_article_matrix.index):\n",
    "            recommended_articles = []\n",
    "            user_langs = significant_languages[user]\n",
    "            sorted_indices = user_predicted_scores[idx].argsort()[::-1]\n",
    "            \n",
    "            user_seen = train_data_random[train_data_random['userID'] == user]['articleID'].values\n",
    "            # 중요 언어 중 소수 언어가 있을 경우 그 언어의 기사를 포함\n",
    "            for lang in user_langs:\n",
    "                minority_articles = article_info[article_info['Language'] == lang]['articleID'].values\n",
    "                # 최소 한 개의 소수 언어 기사를 추가\n",
    "                \n",
    "                for article in minority_articles:\n",
    "                    if article not in user_seen:\n",
    "                        print(f'MINORITY ARTICLE IN {user} <- {article}')\n",
    "                        recommended_articles.append(article)  # 하나의 기사만 추가\n",
    "                        break\n",
    "                    \n",
    "            #---------------------------------\n",
    "            # 기사 수가 충분하지 않은 경우 일반적으로 추천 방식의 기사 추가\n",
    "            while len(recommended_articles) < 5:\n",
    "                for article in user_article_matrix.columns[sorted_indices].tolist():\n",
    "                    if article not in recommended_articles:\n",
    "                        recommended_articles.append(article)\n",
    "                        if len(recommended_articles) == 5:\n",
    "                            break\n",
    "            #---------------------------------\n",
    "            # 유사점 점수를 먼저 추가하는 코드\n",
    "            # for article in global_top_articles:\n",
    "            #     if len(recommended_articles) == 5:\n",
    "            #         break\n",
    "                \n",
    "            #     if article not in recommended_articles:\n",
    "            #         recommended_articles.append(article)\n",
    "                    \n",
    "            # if len(person_reco_dics[user]):\n",
    "            #     for embedding_reco in person_reco_dics[user]:\n",
    "            #         if len(recommended_articles) == 5:\n",
    "            #             recommended_articles.pop()\n",
    "            #         recommended_articles.append(embedding_reco)\n",
    "                \n",
    "            #     if len(recommended_articles) == 5:\n",
    "            #             break\n",
    "                    \n",
    "            #---------------------------------\n",
    "            \n",
    "            #---------------------------------\n",
    "            # 임베딩 점수를 먼저 추가하는 코드\n",
    "            # if len(person_reco_dics[user]):\n",
    "            #     for embedding_reco in person_reco_dics[user]:\n",
    "                    \n",
    "            #         recommended_articles.append(embedding_reco)\n",
    "                    \n",
    "            #         if len(recommended_articles) == 5:\n",
    "            #             break\n",
    "                    \n",
    "            # for article in global_top_articles:\n",
    "            #     if len(recommended_articles) == 5:\n",
    "            #         break\n",
    "                \n",
    "            #     if article not in recommended_articles:\n",
    "            #         recommended_articles.pop()\n",
    "            #         recommended_articles.append(article)\n",
    "                    \n",
    "                \n",
    "            #---------------------------------\n",
    "\n",
    "            recommendations.extend([(user, article) for article in recommended_articles[:5]])\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    # 추천 로직 실행\n",
    "    user_significant_languages_train_random = check_recommended_language(train_data_with_lang_random, minority_languages, lang_thresholds)\n",
    "    test_user_index_random = test_data_random['userID'].unique()\n",
    "    language_filtered_recommendations_random = get_language_filtered_recommendations_with_fallback_and_minority(\n",
    "        user_article_matrix_train_random, user_similarity_train_random, user_significant_languages_train_random, article_info, person_reco_dics, train_data_random\n",
    "    )\n",
    "\n",
    "    # 결과 DataFrame으로 변환\n",
    "    language_filtered_recommendations_random_df = pd.DataFrame(language_filtered_recommendations_random, columns=['userID', 'articleID'])\n",
    "\n",
    "    # Calculate Recall@5 for the modified recommendations including minority language articles\n",
    "    recall_at_5_with_minority = recall5(test_data_for_comparison, language_filtered_recommendations_random_df)\n",
    "    return recall_at_5_with_minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "\n",
    "# 그리드 서치 함수\n",
    "def grid_search(nation_weight_values, visit_log_mins, remain_cnts, seeds, lang_thresholds =[]):\n",
    "    def select_random_articles(group):\n",
    "        max_articles = max(1, min(5, int(len(group) * 0.2)))  # 최소 1개, 최대 10개 또는 전체의 20%\n",
    "        num_articles = np.random.randint(0, max_articles + 1)  # 랜덤 선택 개수 결정\n",
    "        return np.random.choice(group.index, num_articles, replace=False)\n",
    "\n",
    "    results = []\n",
    "    for seed in seeds:\n",
    "        print('START SEED : ', seed)\n",
    "        np.random.seed(seed)\n",
    "        for remain_cnt in tqdm(remain_cnts):\n",
    "            test_drop_idxs = []\n",
    "            test_indices = view_log.drop_duplicates().groupby('userID').apply(select_random_articles).explode().dropna().astype(int)\n",
    "            test_data_random = view_log.loc[test_indices]\n",
    "            train_data_random = view_log.drop(test_indices)\n",
    "            \n",
    "            for u_id in (test_data_random['userID'].values):\n",
    "                test_articles = test_data_random[test_data_random['userID'] == u_id]['articleID'].values\n",
    "                tmp = train_data_random[train_data_random['userID'] == u_id]\n",
    "                test_drop_idxs += tmp[tmp['articleID'].isin(test_articles)].index.to_list()\n",
    "            train_data_random.drop(index = test_drop_idxs)\n",
    "            \n",
    "            user_countries = view_log.groupby('userID')['userCountry'].unique().apply(set)\n",
    "            test_data_for_comparison = test_data_random[['userID', 'articleID']]\n",
    "            \n",
    "            drop_idx = []\n",
    "            for user_id in train_data_random['userID'].unique():\n",
    "                user_writed_articles = article_info[article_info['userID'] == user_id]['articleID'].values\n",
    "                user_view_log = train_data_random[train_data_random['userID'] == user_id]\n",
    "                self_view_df = user_view_log[user_view_log['articleID'].isin(user_writed_articles)]\n",
    "                self_ivew_list = self_view_df.index.to_list()\n",
    "                remain_idx = self_view_df.groupby(['userID','articleID']).head(remain_cnt).index.to_list()\n",
    "                drop_idx += [idx for idx in self_ivew_list if idx not in remain_idx]\n",
    "\n",
    "            train_data_random = train_data_random.drop(index=drop_idx)\n",
    "\n",
    "            seed_results = []\n",
    "            \n",
    "            for nation_weight, visit_log_min in itertools.product(nation_weight_values, visit_log_mins):\n",
    "                user_article_matrix_train_random = train_data_random.groupby(['userID', 'articleID']).size().reset_index(name='visit_count')\n",
    "                user_article_matrix_train_random['visit_count'] = user_article_matrix_train_random['visit_count'].apply(lambda x: min(x, visit_log_min))\n",
    "                user_article_matrix_train_random['log_weight'] = user_article_matrix_train_random['visit_count'].apply(lambda x: np.log1p(x))\n",
    "                user_article_matrix_train_random = user_article_matrix_train_random.pivot(index='userID', columns='articleID', values='log_weight').fillna(0)\n",
    "                # for nation_weight, similarity_threshold, lang_threshold in itertools.product(nation_weight_values, similarity_threshold_values, lang_thresholds):\n",
    "                \n",
    "                user_similarity_train_random = cosine_similarity(user_article_matrix_train_random)\n",
    "\n",
    "                mean_similarity = np.mean(user_similarity_train_random[user_similarity_train_random > 0])\n",
    "                additional_weight = mean_similarity * nation_weight\n",
    "                user_index = user_article_matrix_train_random.index\n",
    "                for i, user_i in enumerate(user_index):\n",
    "                    countries_i = user_countries[user_i]\n",
    "                    for j, user_j in enumerate(user_index[i + 1:], start=i + 1):\n",
    "                        countries_j = user_countries[user_j]\n",
    "                        if countries_i & countries_j:\n",
    "                            user_similarity_train_random[i, j] -= additional_weight\n",
    "                            user_similarity_train_random[j, i] -= additional_weight                    \n",
    "\n",
    "                person_reco_dics = {id: [] for id in train_data_random['userID'].unique()}\n",
    "\n",
    "                current_recall = base_recoomand(user_article_matrix_train_random, user_similarity_train_random, person_reco_dics, test_data_for_comparison, df_return=False)\n",
    "                seed_results.append({\n",
    "                    'seed': seed,\n",
    "                    'nation_weight': nation_weight,\n",
    "                    'visit_log_mins' : visit_log_min,\n",
    "                    'remain_cnts': remain_cnt,\n",
    "                    # 'lang_threshold' : lang_threshold,\n",
    "                    'recall': current_recall\n",
    "                })\n",
    "\n",
    "                results.extend(seed_results)\n",
    "                pd.DataFrame(results).to_csv('pararm29_search_best_params_nationMinus_visitLogMinrange.csv', index=False)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "# Define the range of values for nation_weight and similarity_threshold\n",
    "nation_weight_values = [0.79, 0.8, 1.0]#np.arange(0, 2.01, 0.01)\n",
    "visit_log_mins = [8,9,10,11,12,13,14,15]\n",
    "remain_cnts = [4,10,999]\n",
    "\n",
    "random.seed(777)\n",
    "seeds = random.sample([i for i in range(9999)], 30)\n",
    "\n",
    "\n",
    "\n",
    "# Run grid search\n",
    "grid_search_results = grid_search(nation_weight_values, visit_log_mins, remain_cnts, seeds, \n",
    "                                #   lang_thresholds\n",
    "                                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기사 타이틀간 임베딩 값으로 각 임베딩에 대한 스칼라를 구하고</br>\n",
    "유저가 방문했던 기사들의 벡터 열을 가져와서 합함 -> 여기서 정규화를 어떻게??</br>\n",
    "가져온 열 값을 유저dot기사 메트릭스에 추가 -> 가중치는 얼마?</br>\n",
    "-> 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
